#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DeepSeek API ì•ˆì „ ìµœì í™” ë°°ì¹˜ ë²ˆì—­ê¸° (ê°œì„ íŒ)
- ê´„í˜¸/íŠ¹ìˆ˜ë¬¸ì ë³´í˜¸(ã€Œã€ã€ã€ã€ã€‘ ë“±)
- í”Œë ˆì´ìŠ¤í™€ë” ë³´ì¡´/ë³µì›
- #BATCH_SPLIT ë³´ì •
- 429 ë° content-filter ì˜ˆì™¸ ì²˜ë¦¬ ë³´ê°•
- ë¶ˆí•„ìš”í•œ ì£¼ì„/ë…¸íŠ¸ í›„ì²˜ë¦¬ ì œê±° ê°•í™”
- batch_size ìë™ ê°ì†Œ ì œê±° (ë¶ˆì¼ì¹˜ ë°œìƒ ì‹œ ê°œë³„ fallback ì²˜ë¦¬)
"""

import os
import json
import time
import random
import re
from azure.ai.inference import ChatCompletionsClient
from azure.ai.inference.models import SystemMessage, UserMessage
from azure.core.credentials import AzureKeyCredential

# ---------------------------
# íŒŒì¼ / ì„¤ì •
# ---------------------------
TOKEN_PATH = r"C:\Users\hoho\Desktop\work\deepseek_token.txt"
INPUT_JSON_PATH = r"C:\Users\hoho\Desktop\work\original_texts_for_retranslation.json"
OUTPUT_JSON_PATH = r"C:\Users\hoho\Desktop\work\translated_output-final22525111111122223334444444555.json"
CHECKPOINT_PATH = OUTPUT_JSON_PATH.replace(".json", "_checkpoint.json")
GLOSSARY_PATH = r"C:\Users\hoho\Desktop\work\glossary-japan.json"

# ---------------------------
# DeepSeek (Azure) ì´ˆê¸°í™”
# ---------------------------
with open(TOKEN_PATH, "r", encoding="utf-8") as f:
    lines = [line.strip() for line in f.readlines()]
DEEPSEEK_ENDPOINT = lines[0]
DEEPSEEK_API_KEY = lines[1]
API_VERSION = lines[2]
MODEL_NAME = lines[3]

client = ChatCompletionsClient(
    endpoint=DEEPSEEK_ENDPOINT,
    credential=AzureKeyCredential(DEEPSEEK_API_KEY),
    api_version=API_VERSION
)

# ---------------------------
# ìš©ì–´ì§‘ ë¡œë“œ
# ---------------------------
with open(GLOSSARY_PATH, "r", encoding="utf-8") as f:
    glossary = json.load(f).get("JP_TO_KR", {})

# ---------------------------
# ìœ í‹¸ í•¨ìˆ˜ë“¤
# ---------------------------
def clean_translation(result: str) -> str:
    """ëª¨ë¸ ì‘ë‹µì—ì„œ ë¶ˆí•„ìš”í•œ íƒœê·¸/ì£¼ì„/ì„¤ëª… ì œê±° (ê°•í™” ë²„ì „)"""
    if not result:
        return ""

    text = result

    # <think>...</think> ë¸”ë¡ ì œê±°
    text = re.sub(r'<think>.*?</think>\n?', '', text, flags=re.DOTALL)

    # "**Translation Notes:**" ì´í›„ ì „ë¶€ ì œê±°
    text = re.sub(r'\*\*Translation Notes:\*\*.*', '', text, flags=re.DOTALL)

    # "(Note: ...)" ê°™ì€ ê´„í˜¸ ì£¼ì„ ì œê±°
    text = re.sub(r'\(Note:.*?\)', '', text, flags=re.DOTALL)

    # ì¤„ ë‹¨ìœ„ì—ì„œ "Note:" ë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ì¥ ì œê±°
    text = re.sub(r'^\s*Note:.*$', '', text, flags=re.MULTILINE)

    # "Translation Notes:" ë‹¨ë… íŒ¨í„´ë„ ì œê±°
    text = re.sub(r'Translation Notes:.*', '', text, flags=re.DOTALL)

    return text.strip()

def fix_batch_markers(text: str) -> str:
    """ëª¨ë¸ì´ #BATCH_SPLIT_x# ì„ ë³€í˜•í–ˆì„ ë•Œ ë³´ì •"""
    text = text.replace('ï¼ƒ', '#')  # fullwidth # -> #
    text = re.sub(r'#BATCH[_\s]*SPIT_(\d+)#', r'#BATCH_SPLIT_\1#', text, flags=re.IGNORECASE)
    text = re.sub(r'#BATCH[_\s]*SPLlT_(\d+)#', r'#BATCH_SPLIT_\1#', text, flags=re.IGNORECASE)
    text = re.sub(r'#BATCH[_\s]*SPLIT[_\s]*(\d+)#', r'#BATCH_SPLIT_\1#', text, flags=re.IGNORECASE)
    return text


# ---------------------------
# í”Œë ˆì´ìŠ¤í™€ë” ê¸°ë°˜ ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ (ìˆ˜ì •)
# ---------------------------
def preprocess_text(text: str):
    text = text or ""
    text = text.strip()

    placeholders = {}
    counter = 0

    token_re = re.compile(
        r'(#BATCH_SPLIT_\d+#|#n|#!ALB\([^)]*\)|<color=[^>]*?>|</color>|â€•â€•|â€”â€”|â€”|[ã€Œã€ï½¢ï½£ã€ã€ã€ã€‘ã€ˆã€‰ã€Šã€‹â€œâ€â€˜â€™"â€œâ€])'
    )

    def _repl(m):
        nonlocal counter
        key = f"__PH_{counter}__"
        placeholders[key] = m.group(0)
        counter += 1
        return key

    protected = token_re.sub(_repl, text)

    # glossary ì ìš©
    if glossary:
        for jp, kr in glossary.items():
            protected = protected.replace(jp, kr)

    return protected, placeholders


def postprocess_text(text: str, placeholders: dict):
    # PH_nëŠ” ì¡´ì¬í•˜ë©´ ì œê±°, ëŒ€ì‹  ì›ë˜ íŠ¹ìˆ˜ë¬¸ìëŠ” ë³µì›
    for k, v in placeholders.items():
        text = text.replace(k, v)
    # ë§Œì•½ ì—¬ì „íˆ ë‚¨ì€ PH_n íŒ¨í„´ì´ ìˆìœ¼ë©´ ì œê±°
    text = re.sub(r'__PH_\d+__', '', text)
    return text.strip()




# ---------------------------
# êµ¬ì¡° ë³µì› (#n ê°œìˆ˜ ë§ì¶”ê¸° ë“±)
# ---------------------------
def restore_structure(translated: str, source: str) -> str:
    source_n_count = source.count("#n")
    translated_n_count = translated.count("#n")
    if translated_n_count > source_n_count:
        translated = re.sub(r'#n', '', translated, count=(translated_n_count - source_n_count))
    elif translated_n_count < source_n_count:
        translated += "#n" * (source_n_count - translated_n_count)
    translated_sentences = [s.strip() for s in translated.split("#n") if s.strip()]
    final_sentences = []
    for s in translated_sentences:
        if s not in final_sentences:
            final_sentences.append(s)
    return "#n".join(final_sentences)

# ---------------------------
# ë²ˆì—­ ìš”ì²­ (ë‹¨ì¼ ë°°ì¹˜)
# ---------------------------
def translate_batch_text(batch_text: str) -> str:
    user_prompt = (
        f"{batch_text}\n\n"
        "Translate the content to Korean. Leave any English text unchanged. "
        "Do NOT change tokens that look like __PH_<number>__ or #BATCH_SPLIT_<number># or markers like #n, #!ALB(...), <color=...>. "
        "Preserve punctuation and bracket characters exactly as in the original placeholders. "
        "Output only the translated content; do not add explanations or notes."
    )

    system_msg = SystemMessage(content=(
        "You are a professional translator. Translate Japanese/Chinese to Korean. "
        "Keep English text unchanged. Do not modify placeholders or special markers. "
        "Return ONLY the translated text. Do not include explanations, notes, or comments."
    ))

    retries = 20
    wait_time = 2.0
    for attempt in range(retries):
        try:
            response = client.complete(
                messages=[system_msg, UserMessage(content=user_prompt)],
                max_tokens=4096,
                temperature=0.0,
                top_p=0.1,
                model=MODEL_NAME
            )
            result = response.choices[0].message.content
            result = clean_translation(result)
            result = fix_batch_markers(result)
            return result
        except Exception as e:
            s = str(e)
            if "429" in s or "Too Many Requests" in s:
                print(f"âš ï¸ 429 â†’ {wait_time:.1f}s ëŒ€ê¸° í›„ ì¬ì‹œë„ ({attempt+1}/{retries})")
                time.sleep(wait_time + random.uniform(0, 1))
                wait_time *= 2
                continue
            if "ResponsibleAIPolicyViolation" in s or "content_filter" in s:
                print("âš ï¸ content_filter ë°œìƒ â†’ í”„ë¡¬í”„íŠ¸ ê°„ì†Œí™” í›„ ì¬ì‹œë„")
                try:
                    fallback_prompt = (
                        f"{batch_text}\n\n"
                        "Translate to Korean. Keep English unchanged. "
                        "Do not change placeholders or markers. "
                        "Output only translated text."
                    )
                    response2 = client.complete(
                        messages=[SystemMessage(content="Translate text to Korean. Keep placeholders unchanged."),
                                  UserMessage(content=fallback_prompt)],
                        max_tokens=4096,
                        temperature=0.0,
                        top_p=0.1,
                        model=MODEL_NAME
                    )
                    result = response2.choices[0].message.content
                    result = clean_translation(result)
                    result = fix_batch_markers(result)
                    return result
                except Exception as e2:
                    print(f"âš ï¸ fallback ì‹¤íŒ¨: {e2}")
                    return batch_text
            print(f"âš ï¸ API ì˜¤ë¥˜: {e}")
            return batch_text
    print("âŒ ì¬ì‹œë„ ì‹¤íŒ¨ â†’ ì›ë¬¸ ë°˜í™˜")
    return batch_text

# ---------------------------
# ë°°ì¹˜ ë²ˆì—­ (ë©”ì¸ ë£¨í”„)
# ---------------------------
def batch_translate(json_data, max_batch_size=10):
    keys = list(json_data.keys())
    translated_data = {}

    # ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
    start_index = 0
    if os.path.exists(CHECKPOINT_PATH):
        with open(CHECKPOINT_PATH, "r", encoding="utf-8-sig") as f:
            checkpoint = json.load(f)
            translated_data = checkpoint.get("data", {})
            start_index = checkpoint.get("index", 0)
        print(f"ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°: {start_index}/{len(keys)}")

    total = len(keys)
    i = start_index
    current_batch_size = max_batch_size

    while i < total:
        batch_keys = keys[i:i + current_batch_size]

        batch_text_list = []
        placeholders_list = []
        non_empty_keys = []

        for key in batch_keys:
            text = json_data.get(key, "")
            if not text or not str(text).strip():
                translated_data[key] = text
                continue
            pre_text, placeholders = preprocess_text(str(text))
            idx = len(batch_text_list)
            batch_text_list.append(f"#BATCH_SPLIT_{idx}#\n{pre_text}")
            placeholders_list.append(placeholders)
            non_empty_keys.append(key)

        if not batch_text_list:
            i += len(batch_keys)
            continue

        batch_input = "\n".join(batch_text_list)
        batch_output = translate_batch_text(batch_input)
        batch_output = fix_batch_markers(batch_output)

        split_output = re.split(r'#BATCH_SPLIT_\d+#\n', batch_output)
        split_output = [s.strip() for s in split_output if s.strip()]

        # ë¶„í•  ê°œìˆ˜ ë¶ˆì¼ì¹˜ ì‹œ â†’ ìë™ batch_size ê°ì†Œ ëŒ€ì‹  ê°œë³„ ë²ˆì—­ fallback
        # ë¶„í•  ê°œìˆ˜ ë¶ˆì¼ì¹˜ ì‹œ â†’ ì„ì‹œë¡œ batch_size ì¤„ì—¬ì„œ ì¬ì‹œë„
        if len(split_output) != len(batch_text_list):
            print(f"âš ï¸ split_output({len(split_output)}) != batch({len(batch_text_list)}) â†’ ì„ì‹œ batch_size ì¶•ì†Œ ì¬ì‹œë„")

            # ìµœì†Œ ë‹¨ìœ„ë¡œ ë‚˜ëˆ ì„œ ì¬ë²ˆì—­ (ì˜ˆ: batch_size 5)
            retry_size = max(3, min(5, len(batch_text_list)))  
            for k in range(0, len(batch_text_list), retry_size):
                sub_batch_keys = non_empty_keys[k:k+retry_size]
                sub_batch_texts = batch_text_list[k:k+retry_size]

                sub_input = "\n".join(sub_batch_texts)
                sub_output = translate_batch_text(sub_input)
                sub_output = fix_batch_markers(sub_output)

                sub_split = re.split(r'#BATCH_SPLIT_\d+#\n', sub_output)
                sub_split = [s.strip() for s in sub_split if s.strip()]

                if len(sub_split) != len(sub_batch_texts):
                    print(f"âš ï¸ ì„œë¸Œ ë°°ì¹˜ ë¶ˆì¼ì¹˜ â†’ ê°œë³„ ë²ˆì—­ fallback ì‹¤í–‰")
                    for j, key in enumerate(sub_batch_keys):
                        text = json_data[key]
                        pre_text, placeholders = preprocess_text(str(text))
                        translated = translate_batch_text(pre_text)
                        translated = fix_batch_markers(translated)
                        translated = postprocess_text(translated, placeholders)
                        translated = restore_structure(translated, text)
                        translated_data[key] = translated
                else:
                    for j, key in enumerate(sub_batch_keys):
                        translated_fragment = sub_split[j]
                        translated_fragment = postprocess_text(translated_fragment, placeholders_list[j])
                        translated_fragment = restore_structure(translated_fragment, json_data[key])
                        translated_data[key] = translated_fragment

            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            with open(OUTPUT_JSON_PATH, "w", encoding="utf-8") as f:
                json.dump(translated_data, f, ensure_ascii=False, indent=2)
            with open(CHECKPOINT_PATH, "w", encoding="utf-8") as f:
                json.dump({"index": i + len(batch_keys), "data": translated_data}, f, ensure_ascii=False, indent=2)

            i += len(batch_keys)
            time.sleep(random.uniform(3, 5))

            # âœ… ë‹¤ìŒ ë£¨í”„ì—ì„œëŠ” ë‹¤ì‹œ ì›ë˜ batch_sizeë¡œ ë³µê·€
            current_batch_size = max_batch_size
            continue

        # ì •ìƒ ì²˜ë¦¬
        for j, key in enumerate(non_empty_keys):
            translated_fragment = split_output[j]
            translated_fragment = postprocess_text(translated_fragment, placeholders_list[j])
            translated_fragment = restore_structure(translated_fragment, json_data[key])
            translated_data[key] = translated_fragment

        with open(OUTPUT_JSON_PATH, "w", encoding="utf-8") as f:
            json.dump(translated_data, f, ensure_ascii=False, indent=2)
        with open(CHECKPOINT_PATH, "w", encoding="utf-8") as f:
            json.dump({"index": i + len(batch_keys), "data": translated_data}, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ {i + len(batch_keys)}/{total} ì™„ë£Œ ë° ì €ì¥")
        i += len(batch_keys)
        time.sleep(random.uniform(3, 5))

    if os.path.exists(CHECKPOINT_PATH):
        os.remove(CHECKPOINT_PATH)

    print(f"\nğŸ‰ ì „ì²´ ë²ˆì—­ ì™„ë£Œ! ê²°ê³¼: {OUTPUT_JSON_PATH}")

# ---------------------------
# ì‹¤í–‰
# ---------------------------
if __name__ == "__main__":
    with open(INPUT_JSON_PATH, "r", encoding="utf-8-sig") as f:
        input_json = json.load(f)
    batch_translate(input_json, max_batch_size=10)
